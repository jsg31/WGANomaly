{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGANomaly --  An Improved Wasserstein GAN with an Encoder reversing the Generator for anomaly detection\n",
    "\n",
    "The Improved Wasserstein GAN algorithm can be found in the paper: https://arxiv.org/abs/1704.00028 \n",
    "\n",
    "A documented version of Improved WGAN implementation in Keras can be found in Keras community distribution https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py\n",
    "A similar code is used and an additional model inverses the generator.\n",
    "\n",
    "Here we apply the anomaly detection on Mnist dataset, we consider in this notebook the 0 digit as being abnormal and the others are being normal.\n",
    "\n",
    "First, we train the Improved Wasserstein GAN on digits form 1 to 9. Then an encoder $E$ stacked with the generator $G$ is trained. The reconstitution $||x - G(E(x)||_2$  of the test digit serves as an anomaly detection score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Activation, Dropout, Conv2DTranspose, BatchNormalization\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "from functools import partial\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE_WGAN = 64 #WGAN batch_size for generator and discriminator\n",
    "BATCH_SIZE_ENC_GEN = 64 #WGANoly batch_size for encoding model\n",
    "TRAINING_RATIO = 5 # number of times the discriminator is trained for one step of generator training\n",
    "GRADIENT_PENALTY_WEIGHT = 10 #gradient penalty used in the improved WGAN version\n",
    "LATENT_SPACE_DIM = 128 #Dimension of input of the generator\n",
    "WGAN_EPOCHS = 30 \n",
    "WGANomaly_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mnist dataset:\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28,28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28,28,1)\n",
    "X_train = (X_train.astype(np.float32)) / 256\n",
    "\n",
    "#Mnist without 0 digit for WGAN training:\n",
    "X_train_0 = X_train[y_train!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Labels for critic training:\n",
    "true_label = np.ones((BATCH_SIZE_WGAN, 1), dtype=np.float32)\n",
    "fake_label = -np.ones((BATCH_SIZE_WGAN, 1), dtype=np.float32)\n",
    "dummy_y = np.zeros((BATCH_SIZE_WGAN, 1), dtype=np.float32) # used for penalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks used and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generator of the GAN\n",
    "def make_gen():\n",
    "    input_gen = Input(shape=(LATENT_SPACE_DIM,))\n",
    "    model = Dense(1024)(input_gen)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Dense(128*7*7)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Reshape((7, 7, 128))(model)\n",
    "    model = Conv2DTranspose(128, (5, 5), strides=2, padding='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Convolution2D( 64, (5, 5), padding='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Conv2DTranspose( 64, (5, 5), strides=2, padding='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Convolution2D(1, (5, 5), padding='same', activation='sigmoid')(model)\n",
    "    return Model(input_gen,model)\n",
    "gen=make_gen()\n",
    "gen.summary()\n",
    "\n",
    "#Discriminator de the GAN\n",
    "def make_critic():\n",
    "    input_critic = Input(shape=(28,28,1))\n",
    "    model= Convolution2D(64, (5, 5), padding='same')(input_critic)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Convolution2D(128, (5, 5), kernel_initializer='he_normal',strides=[2, 2])(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Convolution2D(128, (5, 5), kernel_initializer='he_normal', padding='same',strides=[2, 2])(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, kernel_initializer='he_normal')(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Dense(1, kernel_initializer='he_normal')(model)\n",
    "    return Model(input_critic,model)\n",
    "critic = make_critic()\n",
    "critic.summary()\n",
    "\n",
    "#Encoder for WGANomaly\n",
    "def make_encoder():\n",
    "    input_encoder = Input(shape=(28,28,1))\n",
    "    model= Convolution2D(64, (5, 5), padding='same')(input_encoder)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Convolution2D(128, (5, 5), kernel_initializer='he_normal',strides=[2, 2])(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Convolution2D(128, (5, 5), kernel_initializer='he_normal', padding='same',strides=[2, 2])(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, kernel_initializer='he_normal')(model)\n",
    "    model = LeakyReLU()(model)\n",
    "    model = Dense(LATENT_SPACE_DIM, kernel_initializer='he_normal')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    return Model(input_encoder,model)\n",
    "encoder = make_encoder()\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Keras improved wasserstein loss from keras-contrib\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples, gradient_penalty_weight):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                            axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    def _merge_function(self, inputs):\n",
    "        weights = K.random_uniform((BATCH_SIZE_WGAN, 1, 1, 1))\n",
    "        return (weights * inputs[0]) + ((1 - weights) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Generator model:\n",
    "gen.trainable = True\n",
    "critic.trainable = False #We only want to train the generator\n",
    "input_gen_model = Input(shape=(LATENT_SPACE_DIM,))\n",
    "output_gen_model = critic(gen(input_gen_model))\n",
    "gen_model = Model(inputs=[input_gen_model], outputs=[output_gen_model])\n",
    "gen_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9), loss=wasserstein_loss)\n",
    "\n",
    "#Critic model:\n",
    "critic.trainable = True #We only want to train the critic\n",
    "gen.trainable = False\n",
    "\n",
    "input_true = Input(shape=X_train[0].shape)\n",
    "input_latent = Input(shape=(LATENT_SPACE_DIM,))\n",
    "input_false = gen(input_latent)\n",
    "averaged_inputs = RandomWeightedAverage()([input_true, input_false]) #Used for penalization\n",
    "\n",
    "output_false = critic(input_false)\n",
    "output_true = critic(input_true)\n",
    "output_average = critic(averaged_inputs)\n",
    "\n",
    "partial_gp_loss = partial(gradient_penalty_loss,averaged_samples=averaged_inputs,gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty' \n",
    "\n",
    "critic_model = Model(inputs=[input_true, input_latent],outputs=[output_true, output_false,output_average])\n",
    "critic_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),loss=[wasserstein_loss,wasserstein_loss,partial_gp_loss])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "critic_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imroved WGAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def minibatch_train(Xtrain,bacth_size):\n",
    "    index = np.random.randint(0,len(Xtrain),bacth_size)\n",
    "    return Xtrain[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training of improved WGAN:\n",
    "for epoch in range(WGAN_EPOCHS):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    critic_loss = []\n",
    "    generator_loss = []\n",
    "    for i in range(int(X_train_0.shape[0] // (BATCH_SIZE_WGAN ))):\n",
    "        for j in range(TRAINING_RATIO):\n",
    "            train_images = minibatch_train(X_train_0,BATCH_SIZE_WGAN)\n",
    "            noise_critic = np.array([np.random.normal(0,1, LATENT_SPACE_DIM) for i in range(BATCH_SIZE_WGAN)]).astype(np.float32)\n",
    "            critic_loss.append(critic_model.train_on_batch([train_images, noise_critic],\n",
    "                                                                       [true_label, fake_label, dummy_y]))\n",
    "        noise_gen = np.array([np.random.normal(0,1, LATENT_SPACE_DIM) for i in range(BATCH_SIZE_WGAN)]).astype(np.float32)\n",
    "        generator_loss.append(gen_model.train_on_batch(noise_gen, true_label))\n",
    "        print(\"critic loss:\",critic_loss[-1], end = '  ')\n",
    "        print(\"generator_loss:\",generator_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGANomaly model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Encoder model for WGANomaly:\n",
    "gen2= make_gen()\n",
    "encoder.trainable = True\n",
    "gen2.trainable = False\n",
    "\n",
    "Input_encoder = Input(shape=X_train_0.shape[1:])\n",
    "Output_encoder_gen = gen2(encoder(Input_encoder))\n",
    "\n",
    "encoder_model = Model(Input_encoder,Output_encoder_gen)\n",
    "encoder_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),loss='mean_squared_error')\n",
    "gen2.set_weights(gen.get_weights())\n",
    "gen2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGANomaly training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training of WGANomaly after the convergence of the generator:\n",
    "for epochs in range(WGANomaly_EPOCHS):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    WGANomal_loss= []\n",
    "    for ii in range(len(X_train)//BATCH_SIZE_ENC_GEN):\n",
    "        images_batch = minibatch_train(X_train_0,BATCH_SIZE_ENC_GEN)\n",
    "        WGANomal_loss.append(encoder_model.train_on_batch(images_batch,images_batch))\n",
    "        print(\"WGANomaly loss:\", WGANomal_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGANomaly scores\n",
    "The higher the score, the higher the probability that sample is abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_test = np.concatenate((X_train[y_train == 0],X_test[y_test == 0]),axis=0)\n",
    "non_zeros_test = X_test[y_test!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "abnormal_samples_score = []\n",
    "normal_sample_score = []\n",
    "for i in range(len(zeros_test)):\n",
    "    abnormal_samples_score.append(mean_squared_error(encoder_model.predict(zeros_test[i].reshape(1,28,28,1)).reshape(28,28),zeros_test[i].reshape(28,28)))\n",
    "for j in range(len(non_zeros_test)):\n",
    "    normal_samples_score.append(mean_squared_error(encoder_model.predict(non_zeros_test[j].reshape(1,28,28,1)).reshape(28,28),non_zeros_test[j].reshape(28,28)))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
